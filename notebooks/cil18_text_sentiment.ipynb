{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cil18-text-sentiment.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "CKKV8GNzKDiC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Computational Intelligence Lab 2018 -- Text Sentiment Classification\n",
        "### Larissa Laich, Lukas Jendele, Michael Wiegner, Ondrej Skopek\n",
        "Department of Computer Science, ETH Zurich, Switzerland\n",
        "\n",
        "## Project definition\n",
        "The use of microblogging and text messaging as a media of communication has greatly increased over the past 10 years. Such large volumes of data amplifies the need for automatic methods to understand the opinion conveyed in a text.\n",
        "\n",
        "### Resources\n",
        "All the necessary resources (including training data) are available at https://inclass.kaggle.com/c/cil-text-classification-2017\n",
        "\n",
        "### Training Data\n",
        "For this problem, we have acquired 2.5M tweets classified as either positive or negative.\n",
        "\n",
        "### Evaluation Metrics\n",
        "*Classification Accuracy*"
      ]
    },
    {
      "metadata": {
        "id": "ac2aNNSdLQCf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data exploration notebook"
      ]
    },
    {
      "metadata": {
        "id": "5bvXIwzXLTnG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import sklearn as skl\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "63fcawqHRBxs",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Google Drive Code\n",
        "\n",
        "from google.colab import auth\n",
        "import io\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaIoBaseDownload, MediaFileUpload\n",
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "\n",
        "def uploadToDrive(filepath, name, mimeType=\"text/plain\"):\n",
        "  # Upload the file to Drive. See:\n",
        "  #\n",
        "  # https://developers.google.com/drive/v3/reference/files/create\n",
        "  # https://developers.google.com/drive/v3/web/manage-uploads\n",
        "\n",
        "  file_metadata = {\n",
        "    'name': name,\n",
        "    'mimeType': mimeType\n",
        "  }\n",
        "  media = MediaFileUpload(filepath, \n",
        "                          mimetype=mimeType,\n",
        "                          resumable=True)\n",
        "  created = drive_service.files().create(body=file_metadata,\n",
        "                                         media_body=media,\n",
        "                                         fields='id').execute()\n",
        "  print('File ID: %s' % created.get('id'))\n",
        "\n",
        "def downloadFromDrive(file_id, downloaded):\n",
        "  # Download the file we just uploaded.\n",
        "  #\n",
        "  # Replace the assignment below with your file ID\n",
        "  # to download a different file.\n",
        "  #\n",
        "  # A file ID looks like: 1uBtlaggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "\n",
        "  request = drive_service.files().get_media(fileId=file_id)\n",
        "  downloader = MediaIoBaseDownload(downloaded, request)\n",
        "  done = False\n",
        "  last_progress = 0\n",
        "  print(\"Downloading %s:\\t%d%%...\" % (file_id, last_progress))\n",
        "  while done is False:\n",
        "    status, done = downloader.next_chunk()\n",
        "    progress = int(status.progress() * 100)\n",
        "    if last_progress + 10 == progress:\n",
        "      last_progress = progress\n",
        "      print(\"Downloading %s:\\t%d%%...\" % (file_id, last_progress))\n",
        "  print(\"Downloading %s:\\t%d%%...\" % (file_id, 100))\n",
        "\n",
        "\n",
        "def findAndDownload(name):\n",
        "  fname = \"/tmp/\" + name\n",
        "  with open(fname, \"wb\") as f:\n",
        "    results = drive_service.files().list(q='name contains \"' + name + '\"').execute()\n",
        "    if len(results[\"files\"]) <= 0:\n",
        "      raise ValueError(\"No such file found: \" + name)\n",
        "    downloadFromDrive(results[\"files\"][0][\"id\"], f)\n",
        "  return fname"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YLuHracZTOPR",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def read_lines(filename):\n",
        "  content = []\n",
        "  with open(filename) as f:\n",
        "    content = f.readlines()\n",
        "    content = [x.strip() for x in content] # strip and split by space\n",
        "  return content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nzrId6DwR2m4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# File description\n",
        "* `train_pos.txt` and `train_neg.txt` -- a small set of training tweets for each of the two classes.\n",
        "* `train_pos_full.txt` and `train_neg_full.txt` -- a complete set of training tweets for each of the two classes, about 1M tweets per class.\n",
        "* `test_data.txt` -- the test set, that is the tweets for which you have to predict the sentiment label.\n",
        "* `sampleSubmission.csv` -- a sample submission file in the correct format, note that each test tweet is numbered.\n",
        "  * Submission of predictions: -1 = negative prediction, 1 = positive prediction\n",
        "\n",
        "Note that all tweets have been tokenized already, so that the words and punctuation are properly separated by a whitespace."
      ]
    },
    {
      "metadata": {
        "id": "9woZxhqcn0Q3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# TODO: Try more preprocessing: some smileys are not detected (<3), etc; what about case (upper/lower)? Remove URLs?\n",
        "# TODO: Look at RCNN, Lukas thinks it did exactly this thing on this dataset (custom LSTM cell)\n",
        "# TODO: Look at number of unique words (or actually, at the words themselves)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UBPacFPqRLtI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Should finish in about ~10sec each for the small datasets\n",
        "# train_pos_filename=\"train_pos.txt\"\n",
        "# train_neg_filename=\"train_neg.txt\"\n",
        "train_pos_filename=\"train_pos_full.txt\"\n",
        "train_neg_filename=\"train_neg_full.txt\"\n",
        "test_filename=\"test_data.txt\" # TODO: test data\n",
        "train_pos_file=\"/tmp/\" + train_pos_filename\n",
        "train_neg_file=\"/tmp/\" + train_neg_filename\n",
        "X_pos = read_lines(findAndDownload(train_pos_filename))\n",
        "X_neg = read_lines(findAndDownload(train_neg_filename))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fij_KA51TpCh",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Glove Twitter embeddings (TODO: Try different ones)\n",
        "#!wget http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
        "#!unzip glove.twitter.27B.zip\n",
        "# Produces files: glove.twitter.27B.100d.txt  glove.twitter.27B.200d.txt  glove.twitter.27B.25d.txt  glove.twitter.27B.50d.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0wQYmD3XqBMf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "from sklearn.model_selection import train_test_split\n",
        "y_s = [+1] * len(X_pos) + [-1] * len(X_neg)\n",
        "X_s = X_pos + X_neg\n",
        "X_train, X_dev, y_train, y_dev = train_test_split(X_s, y_s, random_state=SEED, test_size=0.8)\n",
        "X_dev, X_test, y_dev, y_test = train_test_split(X_dev, y_dev, random_state=SEED, test_size=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uDiPizHQwLig",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class CILDataset:\n",
        "    \"\"\"Class capable of loading CIL Twitter dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, lines, sentiments, add_bow_eow=False, train=None):\n",
        "        \"\"\"Load dataset from the given files.\n",
        "\n",
        "        Arguments:\n",
        "        add_bow_eow: Whether to add BOW/EOW characters to the word characters.\n",
        "        train: If given, the vocabularies from the training data will be reused.\n",
        "        \"\"\"\n",
        "\n",
        "        # Create vocabulary_maps\n",
        "        if train:\n",
        "            self._vocabulary_maps = train._vocabulary_maps\n",
        "        else:\n",
        "            self._vocabulary_maps = {'chars': {'<pad>': 0, '<unk>': 1, '<bow>': 2, '<eow>': 3},\n",
        "                                     'words': {'<pad>': 0, '<unk>': 1, '\\n': 2}, # \\n represents EOS\n",
        "                                     'tags': {'<pad>': 0, '<unk>': 1, '\\n': 2}, # \\n represents EOS\n",
        "                                     'sentiments': {}}\n",
        "        self._word_ids = []\n",
        "        self._charseq_ids = []\n",
        "        self._charseqs_map = {'<pad>': 0}\n",
        "        self._charseqs = []\n",
        "        self._sentiments = []\n",
        "\n",
        "        # Load the sentences\n",
        "        for idx, line in enumerate(lines):\n",
        "            sentiment = sentiments[idx]\n",
        "            line = line.rstrip(\"\\r\\n\")\n",
        "            if not train:\n",
        "                if sentiment not in self._vocabulary_maps['sentiments']:\n",
        "                    self._vocabulary_maps['sentiments'][sentiment] = len(self._vocabulary_maps['sentiments'])\n",
        "            self._sentiments.append(self._vocabulary_maps['sentiments'][sentiment])\n",
        "\n",
        "            self._word_ids.append([])\n",
        "            self._charseq_ids.append([])\n",
        "            for word_s in line.split(\" \"):\n",
        "                word = word_s if len(word_s) else \"\\n\"\n",
        "\n",
        "                # Characters\n",
        "                if word not in self._charseqs_map:\n",
        "                    self._charseqs_map[word] = len(self._charseqs)\n",
        "                    self._charseqs.append([])\n",
        "                    if add_bow_eow:\n",
        "                        self._charseqs[-1].append(self._vocabulary_maps['chars']['<bow>'])\n",
        "                    for c in word:\n",
        "                        if c not in self._vocabulary_maps['chars']:\n",
        "                            if not train:\n",
        "                                self._vocabulary_maps['chars'][c] = len(self._vocabulary_maps['chars'])\n",
        "                            else:\n",
        "                                c = '<unk>'\n",
        "                        self._charseqs[-1].append(self._vocabulary_maps['chars'][c])\n",
        "                    if add_bow_eow:\n",
        "                        self._charseqs[-1].append(self._vocabulary_maps['chars']['<eow>'])\n",
        "                self._charseq_ids[-1].append(self._charseqs_map[word])\n",
        "\n",
        "                # Words\n",
        "                if word not in self._vocabulary_maps['words']:\n",
        "                    if not train:\n",
        "                        self._vocabulary_maps['words'][word] = len(self._vocabulary_maps['words'])\n",
        "                    else:\n",
        "                        word = '<unk>'\n",
        "                self._word_ids[-1].append(self._vocabulary_maps['words'][word])\n",
        "\n",
        "        # Compute sentence lengths\n",
        "        sentences = len(self._word_ids)\n",
        "        self._sentence_lens = np.zeros([sentences], np.int32)\n",
        "        for i in range(sentences):\n",
        "            self._sentence_lens[i] = len(self._word_ids[i])\n",
        "\n",
        "        # Create vocabularies\n",
        "        if train:\n",
        "            self._vocabularies = train._vocabularies\n",
        "        else:\n",
        "            self._vocabularies = {}\n",
        "            for feature, words in self._vocabulary_maps.items():\n",
        "                self._vocabularies[feature] = [\"\"] * len(words)\n",
        "                for word, id in words.items():\n",
        "                    self._vocabularies[feature][id] = word\n",
        "\n",
        "        self._permutation = np.random.permutation(len(self._sentence_lens))\n",
        "\n",
        "        \n",
        "    def vocabulary(self, feature):\n",
        "        \"\"\"Return vocabulary for required feature.\n",
        "\n",
        "        The features are the following:\n",
        "        words\n",
        "        chars\n",
        "        sentiments\n",
        "        \"\"\"\n",
        "        return self._vocabularies[feature]\n",
        "\n",
        "    def next_batch(self, batch_size):\n",
        "        \"\"\"Return the next batch.\n",
        "\n",
        "        Arguments:\n",
        "        Returns: (sentence_lens, word_ids, charseq_ids, charseqs, charseq_lens, sentiments)\n",
        "        sequence_lens: batch of sentence_lens\n",
        "        word_ids: batch of word_ids\n",
        "        charseq_ids: batch of charseq_ids (the same shape as word_ids, but with the ids pointing into charseqs).\n",
        "        charseqs: unique charseqs in the batch, indexable by charseq_ids;\n",
        "          contain indices of characters from vocabulary('chars')\n",
        "        charseq_lens: length of charseqs\n",
        "        sentiments: batch of sentiments\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = min(batch_size, len(self._permutation))\n",
        "        batch_perm = self._permutation[:batch_size]\n",
        "        self._permutation = self._permutation[batch_size:]\n",
        "        return self._next_batch(batch_perm)\n",
        "\n",
        "    def epoch_finished(self):\n",
        "        if len(self._permutation) == 0:\n",
        "            self._permutation = np.random.permutation(len(self._sentence_lens))\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def whole_data_as_batch(self):\n",
        "        \"\"\"Return the whole dataset in the same result as next_batch.\n",
        "\n",
        "        Returns the same results as next_batch.\n",
        "        \"\"\"\n",
        "        return self._next_batch(np.arange(len(self._sentence_lens)))\n",
        "\n",
        "    def _next_batch(self, batch_perm):\n",
        "        batch_size = len(batch_perm)\n",
        "\n",
        "        # General data\n",
        "        batch_sentence_lens = self._sentence_lens[batch_perm]\n",
        "        max_sentence_len = np.max(batch_sentence_lens)\n",
        "\n",
        "        # Word-level data\n",
        "        batch_word_ids = np.zeros([batch_size, max_sentence_len], np.int32)\n",
        "        for i in range(batch_size):\n",
        "            batch_word_ids[i, 0:batch_sentence_lens[i]] = self._word_ids[batch_perm[i]]\n",
        "        \n",
        "        batch_sentiments = np.zeros([batch_size], np.int32)\n",
        "        for i in range(batch_size):\n",
        "            batch_sentiments[i] = self._sentiments[batch_perm[i]]\n",
        "\n",
        "        # Character-level data\n",
        "        batch_charseq_ids = np.zeros([batch_size, max_sentence_len], np.int32)\n",
        "        charseqs_map, charseqs, charseq_lens = {}, [], []\n",
        "        for i in range(batch_size):\n",
        "            for j, charseq_id in enumerate(self._charseq_ids[batch_perm[i]]):\n",
        "                if charseq_id not in charseqs_map:\n",
        "                    charseqs_map[charseq_id] = len(charseqs)\n",
        "                    charseqs.append(self._charseqs[charseq_id])\n",
        "                batch_charseq_ids[i, j] = charseqs_map[charseq_id]\n",
        "\n",
        "        batch_charseq_lens = np.array([len(charseq) for charseq in charseqs], np.int32)\n",
        "        batch_charseqs = np.zeros([len(charseqs), np.max(batch_charseq_lens)], np.int32)\n",
        "        for i in range(len(charseqs)):\n",
        "            batch_charseqs[i, 0:len(charseqs[i])] = charseqs[i]\n",
        "\n",
        "        return batch_sentence_lens, batch_word_ids, batch_charseq_ids, batch_charseqs, batch_charseq_lens, batch_sentiments\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l96RwXF7pKs_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class Network:\n",
        "    CLASSES = 2\n",
        "\n",
        "    def __init__(self, rnn_cell, rnn_cell_dim, num_words, num_chars, logdir, expname, threads=1, seed=SEED, word_embedding=100, char_embedding=100, keep_prob=0.5, learning_rate=1e-4):\n",
        "        # Create an empty graph and a session\n",
        "        graph = tf.Graph()\n",
        "        graph.seed = seed\n",
        "        self.session = tf.Session(graph=graph, config=tf.ConfigProto(inter_op_parallelism_threads=threads,\n",
        "                                                                     intra_op_parallelism_threads=threads))\n",
        "\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
        "        self.summary_writer = tf.summary.FileWriter(\"{}/{}-{}\".format(logdir, timestamp, expname), flush_secs=10)\n",
        "\n",
        "        # Construct the graph\n",
        "        with self.session.graph.as_default():\n",
        "            if rnn_cell == \"LSTM\":\n",
        "                rnn_cell_co = tf.nn.rnn_cell.LSTMCell(rnn_cell_dim)\n",
        "            elif rnn_cell == \"GRU\":\n",
        "                rnn_cell_co = tf.nn.rnn_cell.GRUCell(rnn_cell_dim)\n",
        "            else:\n",
        "                raise ValueError(\"Unknown rnn_cell {}\".format(rnn_cell))\n",
        "\n",
        "            self.global_step = tf.Variable(0, dtype=tf.int64, trainable=False, name=\"global_step\")\n",
        "            self.sentence_lens = tf.placeholder(tf.int32, [None])\n",
        "            self.word_ids = tf.placeholder(tf.int32, [None, None])\n",
        "            self.charseq_ids = tf.placeholder(tf.int32, [None, None])\n",
        "            self.charseqs = tf.placeholder(tf.int32, [None, None])\n",
        "            self.charseq_lens = tf.placeholder(tf.int32, [None])\n",
        "            self.sentiments = tf.placeholder(tf.int32, [None])\n",
        "            self.is_training = tf.placeholder_with_default(False, [])\n",
        "            self.keep_prob = tf.placeholder_with_default(1.0, [])\n",
        "\n",
        "            rnn_cell_co = tf.nn.rnn_cell.DropoutWrapper(rnn_cell_co, self.keep_prob, self.keep_prob)\n",
        "\n",
        "            if char_embedding == -1:\n",
        "                input_chars = tf.one_hot(self.charseqs, num_chars)\n",
        "            else:\n",
        "                input_chars = tf.nn.embedding_lookup(tf.get_variable(\"char_emb\", shape=[num_chars, char_embedding]),\n",
        "                                                     self.charseqs)\n",
        "            print(\"input_chars\", input_chars.get_shape())\n",
        "\n",
        "            if rnn_cell == \"LSTM\":\n",
        "                rnn_cell_ce = tf.nn.rnn_cell.LSTMCell(rnn_cell_dim)\n",
        "            elif rnn_cell == \"GRU\":\n",
        "                rnn_cell_ce = tf.nn.rnn_cell.GRUCell(rnn_cell_dim)\n",
        "            else:\n",
        "                raise ValueError(\"Unknown rnn_cell {}\".format(rnn_cell))\n",
        "            _, (state_fw, state_bw) = tf.nn.bidirectional_dynamic_rnn(rnn_cell_ce, rnn_cell_ce, input_chars,\n",
        "                                                                          self.charseq_lens, dtype=tf.float32, scope=\"rnn_chars\")\n",
        "            input_chars = state_fw + state_bw\n",
        "            print(\"input_chars\", input_chars.get_shape())\n",
        "\n",
        "            input_char_words = tf.nn.embedding_lookup(input_chars, self.charseq_ids)\n",
        "            print(\"input_char_words\", input_char_words.get_shape())\n",
        "\n",
        "            if word_embedding == -1:\n",
        "                input_words = tf.one_hot(self.word_ids, num_words)\n",
        "            else:\n",
        "                input_words = tf.nn.embedding_lookup(tf.get_variable(\"word_emb\", shape=[num_words, word_embedding]),\n",
        "                                                     self.word_ids)\n",
        "            \n",
        "            # TODO: Add GLOVE\n",
        "            \n",
        "            print(\"input_words\", input_words.get_shape())\n",
        "            inputs = tf.concat([input_char_words, input_words], axis=2)\n",
        "            print(\"inputs\", inputs.get_shape())\n",
        "\n",
        "            (outputs_fw, outputs_bw), (state_fw, state_bw) = tf.nn.bidirectional_dynamic_rnn(rnn_cell_co, rnn_cell_co, inputs, self.sentence_lens, dtype=tf.float32)\n",
        "            states = tf.concat([state_fw, state_bw], axis=1)\n",
        "            print(\"states\", states.get_shape())\n",
        "\n",
        "            hidden = tf.layers.dense(states, 300, activation=tf.nn.leaky_relu)\n",
        "            d1 = tf.layers.dropout(hidden, rate=self.keep_prob)\n",
        "            output_layer = tf.layers.dense(d1, self.CLASSES, activation=None)\n",
        "            print(\"output_layer\", output_layer.get_shape())\n",
        "\n",
        "            self.loss = tf.losses.sparse_softmax_cross_entropy(self.sentiments, output_layer,\n",
        "                                                               reduction=tf.losses.Reduction.SUM_BY_NONZERO_WEIGHTS)\n",
        "            self.training = tf.train.AdamOptimizer(learning_rate).minimize(self.loss, self.global_step)\n",
        "            self.predictions = tf.cast(tf.argmax(output_layer, 1), tf.int32)\n",
        "            self.accuracy = tf.contrib.metrics.accuracy(self.sentiments, self.predictions)\n",
        "\n",
        "            self.train_summary = tf.summary.merge([tf.summary.scalar(\"loss\", self.loss, family=\"train\"),\n",
        "                                                   tf.summary.scalar(\"accuracy\", self.accuracy, family=\"train\")])\n",
        "            self.dev_summary = tf.summary.merge([tf.summary.scalar(\"loss\", self.loss, family=\"dev\"),\n",
        "                                                 tf.summary.scalar(\"accuracy\", self.accuracy, family=\"dev\")])\n",
        "\n",
        "            # Initialize variables\n",
        "            self.session.run(tf.initialize_all_variables())\n",
        "\n",
        "    @property\n",
        "    def training_step(self):\n",
        "        return self.session.run(self.global_step)\n",
        "\n",
        "    def train_epoch(self, data):\n",
        "        while not data.epoch_finished():\n",
        "            sentence_lens, word_ids, charseq_ids, charseqs, charseq_lens, sentiments = \\\n",
        "                data.next_batch(FLAGS.batch_size)\n",
        "            _, summary = \\\n",
        "                self.session.run([self.training, self.train_summary],\n",
        "                                 {self.sentence_lens: sentence_lens, self.word_ids: word_ids,\n",
        "                                  self.charseq_ids: charseq_ids, self.charseqs: charseqs, self.charseq_lens: charseq_lens,\n",
        "                                  self.sentiments: sentiments, self.is_training: True, self.keep_prob: FLAGS.keep_prob})\n",
        "            self.summary_writer.add_summary(summary, self.training_step)\n",
        "            print(\".\", end='')\n",
        "        print(\"\")\n",
        "            \n",
        "\n",
        "    def evaluate_epoch(self, data):\n",
        "        epoch_acc = 0\n",
        "        epoch_loss = 0\n",
        "        cnt = 0\n",
        "        while not data.epoch_finished():\n",
        "            sentence_lens, word_ids, charseq_ids, charseqs, charseq_lens, sentiments = \\\n",
        "                data.next_batch(FLAGS.batch_size)\n",
        "            accuracy, loss, summary = \\\n",
        "                self.session.run([self.accuracy, self.loss, self.dev_summary],\n",
        "                                 {self.sentence_lens: sentence_lens, self.word_ids: word_ids,\n",
        "                                  self.charseq_ids: charseq_ids, self.charseqs: charseqs, self.charseq_lens: charseq_lens,\n",
        "                                  self.sentiments: sentiments})\n",
        "            self.summary_writer.add_summary(summary, self.training_step)\n",
        "            epoch_acc += accuracy\n",
        "            epoch_loss += loss\n",
        "            cnt += 1\n",
        "        # TODO: Proper per epoch summaries for dev\n",
        "        return (epoch_acc / cnt, epoch_loss / cnt)\n",
        "\n",
        "\n",
        "    def predict_epoch(self, data):\n",
        "        predictions = []\n",
        "        while not data.epoch_finished():\n",
        "            sentence_lens, word_ids, charseq_ids, charseqs, charseq_lens, sentiments = \\\n",
        "                data.next_batch(FLAGS.batch_size)\n",
        "            batch_predictions = self.session.run(self.predictions,\n",
        "                                {self.sentence_lens: sentence_lens, self.word_ids: word_ids,\n",
        "                                 self.charseq_ids: charseq_ids, self.charseqs: charseqs, self.charseq_lens: charseq_lens})\n",
        "            predictions.extend(batch_predictions)\n",
        "        return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vHF6sV5cR60F",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "global FLAGS\n",
        "FLAGS = tf.app.flags.FLAGS\n",
        "\n",
        "\n",
        "def define_flags():\n",
        "    def reset_flags():\n",
        "        import argparse as _argparse\n",
        "        tf.app.flags._global_parser = _argparse.ArgumentParser()\n",
        "        \n",
        "    reset_flags()\n",
        "    # Directories\n",
        "    if os.name == 'nt':\n",
        "        tf.app.flags.DEFINE_string('checkpoint_dir', 'e:/temp/tensorflow/checkpoints/',\n",
        "                                   'Directory to save checkpoints in (once per epoch)')\n",
        "    else:\n",
        "        tf.app.flags.DEFINE_string('checkpoint_dir', '/tmp/tensorflow/checkpoints/',\n",
        "                                   'Directory to save checkpoints in (once per epoch)')\n",
        "\n",
        "    # TF parameters\n",
        "    tf.app.flags.DEFINE_boolean(\n",
        "        \"no_gpu\", False, 'Disables GPU usage even if a GPU is available')\n",
        "\n",
        "    # Optimization parameters\n",
        "    tf.app.flags.DEFINE_integer('epochs', 15, 'Training epoch count')\n",
        "    tf.app.flags.DEFINE_integer('batch_size', 1000, 'Training batch size')\n",
        "\n",
        "    # Jupyter notebook params\n",
        "    # Only to avoid raising UnrecognizedFlagError\n",
        "    tf.app.flags.DEFINE_string('f', 'kernel', 'Kernel')\n",
        "    \n",
        "    tf.app.flags.DEFINE_string('logdir', 'logs', 'Logdir name.')\n",
        "    tf.app.flags.DEFINE_string('rnn_cell', \"GRU\", 'RNN cell type.')\n",
        "    tf.app.flags.DEFINE_integer('rnn_cell_dim', 100, 'RNN cell dimension.')\n",
        "    tf.app.flags.DEFINE_integer('threads', 8, 'Maximum number of threads to use.')\n",
        "    tf.app.flags.DEFINE_integer('word_embedding', 100, 'word_embedding')\n",
        "    tf.app.flags.DEFINE_integer('char_embedding', 100, 'char_embedding')\n",
        "    tf.app.flags.DEFINE_float('keep_prob', 0.5, 'dropout probability')\n",
        "    tf.app.flags.DEFINE_float('learning_rate', 1e-4, 'learning rate')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WVmcK2b0SAC5",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Fix random seed\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Parse arguments\n",
        "define_flags()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YF7C00PLXAKC",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# Load the data\n",
        "data_train = CILDataset(X_train, y_train)\n",
        "data_dev = CILDataset(X_dev, y_dev, train=data_train)\n",
        "data_test = CILDataset(X_test, y_test, train=data_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZMK_GzCQYi0c",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Construct the network\n",
        "print(\"Constructing the network.\", file=sys.stderr)\n",
        "expname = \"{}{}-bs{}-epochs{}-char{}-word{}\".format(FLAGS.rnn_cell, FLAGS.rnn_cell_dim, FLAGS.batch_size, FLAGS.epochs, FLAGS.char_embedding, FLAGS.word_embedding)\n",
        "network = Network(rnn_cell=FLAGS.rnn_cell, rnn_cell_dim=FLAGS.rnn_cell_dim,\n",
        "                  num_words=len(data_train.vocabulary('words')), num_chars=len(data_train.vocabulary('chars')),\n",
        "                  logdir=FLAGS.logdir, expname=expname, threads=FLAGS.threads,\n",
        "                  word_embedding=FLAGS.word_embedding, char_embedding=FLAGS.char_embedding,\n",
        "                  keep_prob=FLAGS.keep_prob, learning_rate=FLAGS.learning_rate)\n",
        "\n",
        "# Train\n",
        "best_dev_accuracy = 0\n",
        "test_predictions = None\n",
        "\n",
        "for epoch in range(FLAGS.epochs):\n",
        "    print(\"Training epoch {}\".format(epoch + 1), file=sys.stderr)\n",
        "    network.train_epoch(data_train)\n",
        "    dev_accuracy, dev_loss = network.evaluate_epoch(data_dev)\n",
        "    print(\"Development accuracy after epoch {} is {:.2f}. Dev loss is {:.2f}\".format(epoch + 1, 100. * dev_accuracy, dev_loss), file=sys.stderr)\n",
        "\n",
        "# TODO: Enable predictions best saving\n",
        "#     if dev_accuracy > best_dev_accuracy:\n",
        "#         best_dev_accuracy = dev_accuracy\n",
        "#         sentence_lens, word_ids, charseq_ids, charseqs, charseq_lens, sentiments = \\\n",
        "#             data_test.whole_data_as_batch()\n",
        "#         test_predictions = network.predict_epoch(sentence_lens, word_ids, charseq_ids, charseqs, charseq_lens)\n",
        "\n",
        "# Print test predictions\n",
        "# for prediction in test_predictions:\n",
        "#     print(data_test.vocabulary('sentiments')[prediction])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}